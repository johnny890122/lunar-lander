{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前置作業"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 引入套件\n",
    "\n",
    "gym 這個套件由 OpenAI 所提供，是一套用來開發與比較 Reinforcement Learning 演算法的工具包（toolkit）。\n",
    "\n",
    "而其餘套件則是為了在 Notebook 中繪圖所需要的套件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 匯入 matplotlib 庫，用於繪圖\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 從 IPython 匯入 display，用於在 Jupyter notebook 中顯示圖形\n",
    "from IPython import display\n",
    "\n",
    "# 匯入 numpy 庫，用於數值計算\n",
    "import numpy as np\n",
    "\n",
    "# 匯入 PyTorch，該庫用於深度學習\n",
    "import torch\n",
    "\n",
    "# 從 torch.nn 中匯入神經網路模組\n",
    "import torch.nn as nn\n",
    "\n",
    "# 匯入優化器模組，用於優化神經網路參數\n",
    "import torch.optim as optim\n",
    "\n",
    "# 匯入函數式模組，包含許多常用的深度學習函數\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 從 torch.distributions 中匯入 Categorical，這是用於處理離散機率分佈的模組\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# 匯入 tqdm.notebook，用於在 Jupyter notebook 中顯示進度條\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 543 # Do not change this\n",
    "def fix(env, seed):\n",
    "  # env.seed(seed)\n",
    "  env.action_space.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed(seed)\n",
    "  torch.cuda.manual_seed_all(seed)\n",
    "  np.random.seed(seed)\n",
    "  random.seed(seed)\n",
    "  torch.torch.use_deterministic_algorithms(True)\n",
    "  torch.are_deterministic_algorithms_enabled()\n",
    "  torch.backends.cudnn.benchmark = False\n",
    "  torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立 [Lunar Lander](https://gym.openai.com/envs/LunarLander-v2/) Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“LunarLander-v2” 這個環境是在模擬登月小艇降落在月球表面時的情形。\n",
    "這個任務的目標是讓登月小艇「安全地」降落在兩個黃色旗幟間的平地上。\n",
    "\n",
    "> Landing pad is always at coordinates (0,0).\n",
    "> Coordinates are the first two numbers in state vector.\n",
    "\n",
    "所謂的「環境」其實同時包括了 agent 和 environment。\n",
    "\n",
    "我們利用 `step()` 這個函式讓 agent 行動，而後函式便會回傳 environment 給予的 state 和 reward。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('LunarLander-v2', render_mode=\"rgb_array\")\n",
    "\n",
    "fix(env, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation / State\n",
    "\n",
    "首先，我們可以看看 environment 回傳給 agent 的 observation 究竟是長什麼樣子的資料："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(gym.spaces.box.Box, (8,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(env.observation_space), env.observation_space.shape \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Box(8,)` 說明我們會拿到 8 維的向量作為 observation，其中包含：垂直及水平座標、速度、角度、加速度等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action\n",
    "\n",
    "而在 agent 得到 observation 和 reward 以後，能夠採取的動作有："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Discrete(4)` 說明 agent 可以採取四種離散的行動：\n",
    "- 0 代表不採取任何行動\n",
    "- 2 代表主引擎向下噴射\n",
    "- 1, 3 則是向左右噴射"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step: agent 和 environment 互動。\n",
    "在進行任何操作前，建議先呼叫 `reset()` 函式讓整個「環境」重置。\n",
    "而這個函式同時會回傳「環境」最初始的狀態。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: [ 0.00736828  1.4108332   0.74630535 -0.00388728 -0.00853116 -0.16904932\n",
      "  0.          0.        ]\n",
      "其餘資訊: {}\n"
     ]
    }
   ],
   "source": [
    "initial_state = env.reset()\n",
    "print(\"state:\", initial_state[0])\n",
    "print(\"其餘資訊:\", initial_state[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接著，我們試著從 agent 的四種行動空間中，隨機採取一個行動"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "random_action = env.action_space.sample()\n",
    "print(random_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再利用 `step()` 函式讓 agent 根據我們隨機抽樣出來的 `random_action` 動作。\n",
    "而這個函式會回傳四項資訊：\n",
    "- observation / state\n",
    "- reward\n",
    "- 完成與否\n",
    "- 其餘資訊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, reward, done, truncate, info = env.step(random_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward\n",
    "\n",
    "而「環境」給予的 reward 大致是這樣計算：\n",
    "- 小艇墜毀得到 -100 分\n",
    "- 小艇在黃旗幟之間成功著地則得 100~140 分\n",
    "- 噴射主引擎（向下噴火）每次 -0.3 分\n",
    "- 小艇最終完全靜止則再得 100 分\n",
    "- 小艇每隻腳碰觸地面 +10 分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.0899525086657307\n"
     ]
    }
   ],
   "source": [
    "print(reward) # after doing a random action (0), the immediate reward is stored in this "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "img = plt.imshow(env.render())\n",
    "\n",
    "done = False\n",
    "cnt = 0\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, truncate, _ = env.step(action)\n",
    "    img.set_data(env.render())\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    if cnt < 10:\n",
    "        cnt += 1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient\n",
    "\n",
    "搭建簡單的 policy network。\n",
    "我們預設模型的輸入是 8-dim 的 observation，輸出則是離散的四個動作之一："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradientNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(8, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, 4)\n",
    "\n",
    "    def forward(self, state):\n",
    "        hid = torch.tanh(self.fc1(state))\n",
    "        hid = torch.tanh(self.fc2(hid))\n",
    "        return F.softmax(self.fc3(hid), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再來，搭建一個簡單的 agent，並搭配上方的 policy network 來採取行動。\n",
    "這個 agent 能做到以下幾件事：\n",
    "- `learn()`：從記下來的 log probabilities 及 rewards 來更新 policy network。\n",
    "- `sample()`：從 environment 得到 observation 之後，利用 policy network 得出應該採取的行動。\n",
    "而此函式除了回傳抽樣出來的 action，也會回傳此次抽樣的 log probabilities。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradientAgent():\n",
    "    \n",
    "    def __init__(self, network):\n",
    "        self.network = network\n",
    "        self.optimizer = optim.SGD(self.network.parameters(), lr=0.001)\n",
    "         \n",
    "    def forward(self, state):\n",
    "        return self.network(state)\n",
    "\n",
    "    def learn(self, log_probs, rewards):\n",
    "        loss = (-log_probs * rewards).sum() # You don't need to revise this to pass simple baseline (but you can)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def sample(self, state):\n",
    "        action_prob = self.network(torch.FloatTensor(state))\n",
    "        action_dist = Categorical(action_prob)\n",
    "        action = action_dist.sample()\n",
    "        log_prob = action_dist.log_prob(action)\n",
    "        return action.item(), log_prob\n",
    "\n",
    "    def save(self, PATH): \n",
    "        Agent_Dict = {\n",
    "            \"network\" : self.network.state_dict(),\n",
    "            \"optimizer\" : self.optimizer.state_dict()\n",
    "        }\n",
    "        torch.save(Agent_Dict, PATH)\n",
    "\n",
    "    def load(self, PATH): \n",
    "        checkpoint = torch.load(PATH)\n",
    "        self.network.load_state_dict(checkpoint[\"network\"])\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = PolicyGradientNetwork()\n",
    "agent = PolicyGradientAgent(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練 Agent\n",
    "\n",
    "現在我們開始訓練 agent。\n",
    "透過讓 agent 和 environment 互動，我們記住每一組對應的 log probabilities 及 reward，並在成功登陸或者不幸墜毀後，回放這些「記憶」來訓練 policy network。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.00229616,  1.4003068 ,  0.23256728, -0.47170618, -0.00265395,\n",
       "        -0.05267997,  0.        ,  0.        ], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = env.reset()\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "343ab5f916cb4ed3856152f082d4781f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards looks like  (513,)\n",
      "logs prob looks like  torch.Size([513])\n",
      "torch.from_numpy(rewards) looks like  torch.Size([513])\n",
      "rewards looks like  (518,)\n",
      "logs prob looks like  torch.Size([518])\n",
      "torch.from_numpy(rewards) looks like  torch.Size([518])\n",
      "rewards looks like  (486,)\n",
      "logs prob looks like  torch.Size([486])\n",
      "torch.from_numpy(rewards) looks like  torch.Size([486])\n",
      "rewards looks like  (510,)\n",
      "logs prob looks like  torch.Size([510])\n",
      "torch.from_numpy(rewards) looks like  torch.Size([510])\n",
      "rewards looks like  (521,)\n",
      "logs prob looks like  torch.Size([521])\n",
      "torch.from_numpy(rewards) looks like  torch.Size([521])\n",
      "rewards looks like  (433,)\n",
      "logs prob looks like  torch.Size([433])\n",
      "torch.from_numpy(rewards) looks like  torch.Size([433])\n",
      "rewards looks like  (460,)\n",
      "logs prob looks like  torch.Size([460])\n",
      "torch.from_numpy(rewards) looks like  torch.Size([460])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m     action, log_prob \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39msample(state) \u001b[38;5;66;03m# at , log(at|st)\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     log_probs\u001b[38;5;241m.\u001b[39mappend(log_prob) \u001b[38;5;66;03m# [log(a1|s1), log(a2|s2), ...., log(at|st)]\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# seq_rewards.append(reward)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/simple_rl/lib/python3.10/site-packages/gym/wrappers/time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/anaconda3/envs/simple_rl/lib/python3.10/site-packages/gym/wrappers/order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/simple_rl/lib/python3.10/site-packages/gym/wrappers/env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/simple_rl/lib/python3.10/site-packages/gym/envs/box2d/lunar_lander.py:556\u001b[0m, in \u001b[0;36mLunarLander.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    545\u001b[0m     p\u001b[38;5;241m.\u001b[39mApplyLinearImpulse(\n\u001b[1;32m    546\u001b[0m         (ox \u001b[38;5;241m*\u001b[39m SIDE_ENGINE_POWER \u001b[38;5;241m*\u001b[39m s_power, oy \u001b[38;5;241m*\u001b[39m SIDE_ENGINE_POWER \u001b[38;5;241m*\u001b[39m s_power),\n\u001b[1;32m    547\u001b[0m         impulse_pos,\n\u001b[1;32m    548\u001b[0m         \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    549\u001b[0m     )\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlander\u001b[38;5;241m.\u001b[39mApplyLinearImpulse(\n\u001b[1;32m    551\u001b[0m         (\u001b[38;5;241m-\u001b[39mox \u001b[38;5;241m*\u001b[39m SIDE_ENGINE_POWER \u001b[38;5;241m*\u001b[39m s_power, \u001b[38;5;241m-\u001b[39moy \u001b[38;5;241m*\u001b[39m SIDE_ENGINE_POWER \u001b[38;5;241m*\u001b[39m s_power),\n\u001b[1;32m    552\u001b[0m         impulse_pos,\n\u001b[1;32m    553\u001b[0m         \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    554\u001b[0m     )\n\u001b[0;32m--> 556\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworld\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mFPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlander\u001b[38;5;241m.\u001b[39mposition\n\u001b[1;32m    559\u001b[0m vel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlander\u001b[38;5;241m.\u001b[39mlinearVelocity\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.network.train()  # 訓練前，先確保 network 處在 training 模式\n",
    "EPISODE_PER_BATCH = 5  # 每蒐集 5 個 episodes 更新一次 agent\n",
    "NUM_BATCH = 400        # 總共更新 400 次\n",
    "\n",
    "avg_total_rewards, avg_final_rewards = [], []\n",
    "\n",
    "prg_bar = tqdm(range(NUM_BATCH))\n",
    "for batch in prg_bar:\n",
    "\n",
    "    log_probs, rewards = [], []\n",
    "    total_rewards, final_rewards = [], []\n",
    "\n",
    "    # 蒐集訓練資料\n",
    "    for episode in range(EPISODE_PER_BATCH):\n",
    "        \n",
    "        state, _ = env.reset()\n",
    "        total_reward, total_step = 0, 0\n",
    "        seq_rewards = []\n",
    "        while True:\n",
    "\n",
    "            action, log_prob = agent.sample(state) # at , log(at|st)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            log_probs.append(log_prob) # [log(a1|s1), log(a2|s2), ...., log(at|st)]\n",
    "            # seq_rewards.append(reward)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            total_step += 1\n",
    "            rewards.append(reward) #改這裡\n",
    "            # ! 重要 ！\n",
    "            # 現在的reward 的implementation 為每個時刻的瞬時reward, 給定action_list : a1, a2, a3 ......\n",
    "            #                                                       reward :     r1, r2 ,r3 ......\n",
    "            # medium：將reward調整成accumulative decaying reward, 給定action_list : a1,                         a2,                           a3 ......\n",
    "            #                                                       reward :     r1+0.99*r2+0.99^2*r3+......, r2+0.99*r3+0.99^2*r4+...... ,r3+0.99*r4+0.99^2*r5+ ......\n",
    "            # boss : implement DQN\n",
    "            if done:\n",
    "                final_rewards.append(reward)\n",
    "                total_rewards.append(total_reward)\n",
    "                break\n",
    "\n",
    "    print(f\"rewards looks like \", np.shape(rewards))  \n",
    "    # print(f\"log_probs looks like \", np.shape(log_probs))     \n",
    "    # 紀錄訓練過程\n",
    "    avg_total_reward = sum(total_rewards) / len(total_rewards)\n",
    "    avg_final_reward = sum(final_rewards) / len(final_rewards)\n",
    "    avg_total_rewards.append(avg_total_reward)\n",
    "    avg_final_rewards.append(avg_final_reward)\n",
    "    prg_bar.set_description(f\"Total: {avg_total_reward: 4.1f}, Final: {avg_final_reward: 4.1f}\")\n",
    "\n",
    "    # 更新網路\n",
    "    # rewards = np.concatenate(rewards, axis=0)\n",
    "    rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-9)  # 將 reward 正規標準化\n",
    "    agent.learn(torch.stack(log_probs), torch.from_numpy(rewards))\n",
    "    print(\"logs prob looks like \", torch.stack(log_probs).size())\n",
    "    print(\"torch.from_numpy(rewards) looks like \", torch.from_numpy(rewards).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
